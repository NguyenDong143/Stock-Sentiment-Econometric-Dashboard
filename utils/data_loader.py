from datetime import datetime
import os
import pandas as pd
import streamlit as st
import logging
# import investpy # ƒê√É B·ªä LO·∫†I B·ªé
import yfinance as yf 
from typing import Optional, Dict
from utils.vndirect_api import get_vndirect_api 

# üÜï VNSTOCK - Lazy loading ƒë·ªÉ tr√°nh l·ªói circular import
_vnstock_module = None

def _get_vnstock():
    """Lazy load vnstock module to avoid circular import issues with vnai."""
    global _vnstock_module
    if _vnstock_module is None:
        try:
            import warnings
            warnings.filterwarnings('ignore', message='pkg_resources is deprecated')
            from vnstock import Vnstock
            _vnstock_module = Vnstock
        except Exception as e:
            logger.error(f"Failed to import vnstock: {e}")
            _vnstock_module = False  # Mark as failed to avoid retrying
    return _vnstock_module if _vnstock_module is not False else None


logger = logging.getLogger(__name__)


# ======================================================
# üîß H√ÄM ƒê·ªåC FILE EXCEL AN TO√ÄN & CHU·∫®N H√ìA D·ªÆ LI·ªÜU
# ======================================================
@st.cache_data(show_spinner=False)
def _safe_load_excel(path: str) -> pd.DataFrame:
    """ƒê·ªçc file Excel an to√†n, chu·∫©n h√≥a t√™n c·ªôt, ki·ªÉu d·ªØ li·ªáu v√† tr√°nh l·ªói Arrow."""
    if not os.path.exists(path):
        st.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: `{path}`")
        logger.warning(f"File kh√¥ng t·ªìn t·∫°i: {path}")
        return pd.DataFrame()

    try:
        # S·ª≠ d·ª•ng engine "openpyxl" l√† ti√™u chu·∫©n cho Streamlit
        df = pd.read_excel(path, engine="openpyxl") 
    except Exception as e:
        st.error(f"‚ùå L·ªói ƒë·ªçc file `{path}`: {e}")
        logger.error(f"L·ªói ƒë·ªçc file {path}: {e}")
        return pd.DataFrame()

    # üîπ Chu·∫©n h√≥a t√™n c·ªôt ‚Äî ch·ªØ th∆∞·ªùng, lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    df.columns = [str(c).strip().lower() for c in df.columns]

    # üîπ Chu·∫©n h√≥a c·ªôt 'date' n·∫øu c√≥
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.sort_values("date").dropna(subset=["date"])

    # üîπ L√†m s·∫°ch d·ªØ li·ªáu s·ªë: thay ',' b·∫±ng '.', x√≥a '%', v√† √©p ki·ªÉu an to√†n
    for col in df.columns:
        if df[col].dtype == "object":
            # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i s·ªë v√† chu·∫©n h√≥a d·∫•u th·∫≠p ph√¢n
            df[col] = (
                df[col]
                .astype(str)
                .str.replace(",", "", regex=False) # X√≥a d·∫•u ph·∫©y ph√¢n c√°ch h√†ng ngh√¨n
                .str.replace(".", "", regex=False) # S·∫Ω thay th·∫ø l·∫°i n·∫øu c·∫ßn
                .str.replace("%", "", regex=False)
                .str.replace("‚Ç´", "", regex=False)
                .str.replace("vnƒë", "", regex=False)
                .str.replace("$", "", regex=False)
                .str.strip()
            )
            # Th·ª≠ √©p ki·ªÉu s·ªë
            try:
                # √âp ki·ªÉu an to√†n, NaN n·∫øu th·∫•t b·∫°i
                df[col] = pd.to_numeric(df[col], errors='coerce')
            except Exception:
                pass

    # üîπ √âp c√°c c·ªôt kh√¥ng ph·∫£i s·ªë v·ªÅ string (fix l·ªói Arrow / Streamlit caching)
    for col in df.select_dtypes(include=["object"]).columns:
        df[col] = df[col].astype(str)

    # üîπ Ki·ªÉm tra b·∫Øt bu·ªôc c√°c c·ªôt c·∫£m x√∫c (n·∫øu c√≥)
    required_cols = ["t√≠ch c·ª±c", "ti√™u c·ª±c", "trung t√≠nh"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        logger.warning(f"Thi·∫øu c√°c c·ªôt {missing} trong file {path}")

    return df


# ======================================================
# üì∞ T·∫¢I D·ªÆ LI·ªÜU C·∫¢M X√öC THEO C·∫§U H√åNH SIDEBAR
# ======================================================
@st.cache_data(show_spinner=False, ttl=7200)
def load_sentiment_data(
    ticker: Optional[str] = None, data_type: str = "Content", time_period: str = "Before Scandal"
) -> pd.DataFrame:
    """
    T·∫£i d·ªØ li·ªáu c·∫£m x√∫c d·ª±a tr√™n c·∫•u h√¨nh ƒë∆∞·ª£c ch·ªçn trong sidebar.
    (Gi·ªØ nguy√™n)
    """
    base_dir = "data"
    type_map = {"Content": "vnecon", "Title": "vnecon_title"}
    period_map = {"Before Scandal": "before_scandals", "After Scandal": "after_scandals"}

    folder_name = f"{type_map.get(data_type, 'vnecon')}_{period_map.get(time_period, 'before_scandals')}"
    data_dir = os.path.join(base_dir, folder_name)

    if not os.path.exists(data_dir):
        st.error(f"‚ùå Th∆∞ m·ª•c d·ªØ li·ªáu kh√¥ng t·ªìn t·∫°i: `{data_dir}`")
        return pd.DataFrame()

    # N·∫øu ng∆∞·ªùi d√πng ch·ªçn m√£ c·ª• th·ªÉ
    if ticker:
        file_path = os.path.join(data_dir, f"{ticker.upper()}.xlsx")
        if os.path.exists(file_path):
            df = _safe_load_excel(file_path)
            return df
        else:
            st.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file `{ticker}.xlsx` trong `{folder_name}/`.")
            return pd.DataFrame()

    # N·∫øu kh√¥ng c√≥ ticker -> h·ª£p nh·∫•t to√†n b·ªô file trong th∆∞ m·ª•c
    dfs = []
    for file in os.listdir(data_dir):
        if file.endswith((".xlsx", ".xls")) and not file.startswith("~$"):
            df = _safe_load_excel(os.path.join(data_dir, file))
            if not df.empty:
                df["ticker"] = file.replace(".xlsx", "").replace(".xls", "").upper()
                dfs.append(df)

    if not dfs:
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file Excel n√†o trong `{folder_name}/`.")
        return pd.DataFrame()

    st.info(f"üìò ƒê√£ h·ª£p nh·∫•t d·ªØ li·ªáu trong `{folder_name}/` ({len(dfs)} file).")
    return pd.concat(dfs, ignore_index=True)


# ======================================================
# üíπ T·∫¢I D·ªÆ LI·ªÜU GI√Å C·ªî PHI·∫æU L·ªäCH S·ª¨ (VNSTOCK)
# ======================================================
@st.cache_data(show_spinner=False, ttl=7200)
def load_price_data(ticker: str) -> pd.DataFrame:
    """
    L·∫•y d·ªØ li·ªáu gi√° c·ªï phi·∫øu l·ªãch s·ª≠ qua Vnstock API.
    ∆Øu ti√™n t·∫£i t·ª´ cache CSV tr∆∞·ªõc.
    """
    Vnstock = _get_vnstock()
    if Vnstock is None:
        return pd.DataFrame()
        
    ticker = ticker.upper()
    path = f"data/prices/{ticker}_vnstock.csv" # ƒê·ªïi t√™n cache ƒë·ªÉ tr√°nh xung ƒë·ªôt
    
    # Ng√†y b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c (YYYY-MM-DD) - vnstock d√πng ƒë·ªãnh d·∫°ng n√†y
    start_date_str = "2018-01-01"
    end_date_str = datetime.now().strftime("%Y-%m-%d")
    
    # üîπ Danh s√°ch m√£ ƒë√£ b·ªã delisted (Ng√†y h·ªßy ni√™m y·∫øt ch√≠nh th·ª©c DD/MM/YYYY)
    delisted_info = {
        'FLC': '05/09/2023', 
        'GAB': '01/03/2024',
        'HAI': '01/08/2023',
    }
    ticker_upper = ticker.upper()

    # 1. T·∫£i t·ª´ cache local
    if os.path.exists(path):
        try:
            df = pd.read_csv(path, index_col='date', parse_dates=True)
            # Ki·ªÉm tra xem cache c√≥ c·∫ßn c·∫≠p nh·∫≠t kh√¥ng
            if df.index.max().date() == datetime.now().date():
                return df
            # N·∫øu kh√¥ng, ti·∫øp t·ª•c t·∫£i m·ªõi
            os.remove(path)
        except Exception:
            os.remove(path)

    # 2. T·∫£i t·ª´ Vnstock v·ªõi retry logic
    df = None
    max_retries = 2
    sources = ['VCI', 'TCBS']  # Th·ª≠ nhi·ªÅu ngu·ªìn
    
    for attempt in range(max_retries):
        for source in sources:
            try:
                stock = Vnstock().stock(symbol=ticker, source=source)
                df = stock.quote.history(start=start_date_str, end=end_date_str)

                if df is not None and not df.empty:
                    # N·∫øu c√≥ d·ªØ li·ªáu, tho√°t kh·ªèi v√≤ng l·∫∑p
                    break
            except Exception as e:
                logger.warning(f"L·∫ßn th·ª≠ {attempt + 1}: L·ªói t·∫£i t·ª´ {source}: {str(e)[:100]}")
                continue
        
        if df is not None and not df.empty:
            break  # ƒê√£ c√≥ d·ªØ li·ªáu, tho√°t v√≤ng ngo√†i
    
    if df is None or df.empty:
        st.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho {ticker} tr√™n Vnstock sau {max_retries} l·∫ßn th·ª≠ v·ªõi {len(sources)} ngu·ªìn.")
        return pd.DataFrame()

    # üîπ Chu·∫©n h√≥a t√™n c·ªôt
    df.rename(columns={
        "time": "date", # C·ªôt time th√†nh date
        "open": "open",
        "high": "high",
        "low": "low",
        "close": "close",
        "volume": "volume"
    }, inplace=True)
    
    # Th√™m Adj Close (t·∫°m th·ªùi b·∫±ng Close n·∫øu kh√¥ng c√≥ s·∫µn)
    if 'adj_close' not in df.columns:
        df['adj_close'] = df['close']
        
    # Ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt v√† ƒë·∫£m b·∫£o th·ª© t·ª±
    required_cols = ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume']
    df = df[[col for col in required_cols if col in df.columns]].copy()

    # ƒê·∫£m b·∫£o c·ªôt date l√† datetime
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values('date').reset_index(drop=True)
    
    # üîπ SET DATE L√ÄM INDEX (Quan tr·ªçng cho bi·ªÉu ƒë·ªì)
    df = df.set_index('date')
    
    # üîπ L√ÄM S·∫†CH D·ªÆ LI·ªÜU M·∫†NH M·∫º: Lo·∫°i b·ªè gi√° = 0 ho·∫∑c flatline sau delisting
    df = df[df['close'] > 0].copy()
    df = df.dropna(subset=['close', 'open', 'high', 'low'])
    
    # üîπ L·ªåC M√É B·ªä DELISTED: Ch·ªâ gi·ªØ d·ªØ li·ªáu ƒë·∫øn ng√†y delisting
    if ticker_upper in delisted_info:
        delisting_date_str = delisted_info[ticker_upper]
        # Chuy·ªÉn ƒë·ªïi DD/MM/YYYY sang datetime
        delisting_date = datetime.strptime(delisting_date_str, "%d/%m/%Y")
        
        # Ch·ªâ gi·ªØ l·∫°i c√°c h√†ng c√≥ ng√†y <= ng√†y delisting (s·ª≠ d·ª•ng index)
        df = df[df.index <= delisting_date].copy()
        logger.info(f"L·ªçc d·ªØ li·ªáu {ticker} ƒë·∫øn ng√†y delisting: {delisting_date.date()}")
        
    # üîπ KI·ªÇM TRA T√çNH CH√çNH X√ÅC (Abnormal changes)
    if len(df) > 0:
        # Gi√° ƒë∆∞·ª£c tr·∫£ v·ªÅ t·ª´ Vnstock th∆∞·ªùng ƒë√£ ƒë∆∞·ª£c nh√¢n 1000/10000 t√πy ngu·ªìn, 
        # nh∆∞ng ph·∫ßn trƒÉm thay ƒë·ªïi v·∫´n ch√≠nh x√°c.
        price_change = df['close'].pct_change().abs()
        abnormal_days = price_change[price_change > 0.5]
        if len(abnormal_days) > 0:
            logger.warning(f"{ticker}: Ph√°t hi·ªán {len(abnormal_days)} ng√†y c√≥ bi·∫øn ƒë·ªông gi√° >50%")
    
    # L∆∞u v√†o cache local (gi·ªØ l·∫°i index date)
    os.makedirs("data/prices", exist_ok=True)
    df.to_csv(path, index=True)
    
    logger.info(f"‚úÖ ƒê√£ t·∫£i {len(df)} ng√†y d·ªØ li·ªáu cho {ticker} t·ª´ Vnstock")
    return df


# ======================================================
# üíπ T·∫¢I D·ªÆ LI·ªÜU GI√Å REAL-TIME (C√ÇU CH·∫§P)
# ======================================================
@st.cache_data(ttl=5, show_spinner=False) # Cache 5 gi√¢y ƒë·ªÉ c·∫≠p nh·∫≠t
def load_realtime_price_quote(ticker: str) -> Optional[Dict]:
    """
    L·∫•y d·ªØ li·ªáu gi√° real-time (last trade quote) t·ª´ VNDirect API.
    S·ª≠ d·ª•ng cho hi·ªÉn th·ªã ti√™u ƒë·ªÅ v√† s·ªë li·ªáu ch√≠nh tr√™n Dashboard.
    (Gi·ªØ nguy√™n)
    """
    if not ticker:
        return None
        
    try:
        vnd_api = get_vndirect_api()
        # H√†m get_stock_price() ƒë√£ bao g·ªìm logic x·ª≠ l√Ω timeout
        return vnd_api.get_stock_price(ticker) 
    except Exception as e:
        logger.error(f"L·ªói t·∫£i gi√° real-time cho {ticker}: {e}")
        return None


# ======================================================
# üîÅ T·∫¢I D·ªÆ LI·ªÜU KI·ªÇM ƒê·ªäNH GRANGER/TVAR THEO C·∫§U H√åNH SIDEBAR
# ======================================================
@st.cache_data(show_spinner=False, ttl=7200)
def load_granger_data(
    ticker: Optional[str] = None, data_type: str = "Content", time_period: str = "Before Scandal"
) -> pd.DataFrame:
    """
    T·∫£i d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a (th∆∞·ªùng l√† Log Return Price v√† Sentiment Score)
    cho c√°c m√¥ h√¨nh Kinh t·∫ø l∆∞·ª£ng (Granger, TVAR).
    (Gi·ªØ nguy√™n)
    """
    base_dir = "data"
    # C√°c th∆∞ m·ª•c n√†y ch·ª©a d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω cho m√¥ h√¨nh kinh t·∫ø l∆∞·ª£ng
    type_map = {"Content": "data", "Title": "data_title"} 
    period_map = {"Before Scandal": "before_scandals", "After Scandal": "after_scandals"}

    folder_name = f"{type_map.get(data_type, 'data')}_{period_map.get(time_period, 'before_scandals')}"
    data_dir = os.path.join(base_dir, folder_name)

    if not os.path.exists(data_dir):
        st.error(f"‚ùå Th∆∞ m·ª•c d·ªØ li·ªáu kh√¥ng t·ªìn t·∫°i: `{data_dir}`")
        logger.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {data_dir}")
        return pd.DataFrame()

    # N·∫øu ng∆∞·ªùi d√πng ch·ªçn m√£ c·ªï phi·∫øu c·ª• th·ªÉ
    if ticker:
        file_path = os.path.join(data_dir, f"{ticker.upper()}.xlsx")
        if os.path.exists(file_path):
            df = _safe_load_excel(file_path)
            return df
        else:
            st.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file `{ticker}.xlsx` trong `{folder_name}/`.")
            logger.warning(f"Thi·∫øu file: {file_path}")
            return pd.DataFrame()

    # N·∫øu kh√¥ng c√≥ ticker ‚Üí h·ª£p nh·∫•t to√†n b·ªô file trong th∆∞ m·ª•c
    dfs = []
    for file in os.listdir(data_dir):
        if file.endswith((".xlsx", ".xls")) and not file.startswith("~$"):
            df = _safe_load_excel(os.path.join(data_dir, file))
            if not df.empty:
                df["ticker"] = file.replace(".xlsx", "").replace(".xls", "").upper()
                dfs.append(df)

    if not dfs:
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file Excel n√†o trong `{folder_name}/`.")
        logger.error(f"Kh√¥ng c√≥ file Excel trong th∆∞ m·ª•c {data_dir}")
        return pd.DataFrame()

    st.info(f"üìä ƒê√£ h·ª£p nh·∫•t d·ªØ li·ªáu Granger trong `{folder_name}/` ({len(dfs)} file).")
    return pd.concat(dfs, ignore_index=True)