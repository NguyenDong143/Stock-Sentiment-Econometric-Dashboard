# ğŸ¤– PHÃ‚N TÃCH CHI TIáº¾T CÃCH PHOBERT HOáº T Äá»˜NG

### ğŸ“š **1. Tá»”NG QUAN Vá»€ PHOBERT**

#### **A. PhoBERT lÃ  gÃ¬?**

**PhoBERT** (Vietnamese BERT) lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ dá»±a trÃªn kiáº¿n trÃºc **BERT (Bidirectional Encoder Representations from Transformers)** Ä‘Æ°á»£c **VinAI Research** phÃ¡t triá»ƒn Ä‘áº·c biá»‡t cho **tiáº¿ng Viá»‡t**.

```
BERT (Google 2018) 
    â†“ Pre-training trÃªn corpus tiáº¿ng Viá»‡t
PhoBERT-base (VinAI 2020)
    â†“ Fine-tuning cho Sentiment Analysis
wonrax/phobert-base-vietnamese-sentiment (Model báº¡n Ä‘ang dÃ¹ng)
```

---

### ğŸ—ï¸ **2. KIáº¾N TRÃšC PHOBERT**

#### **A. Cáº¥u trÃºc Transformer**

```
Input Text
    â†“
[Tokenization] â†’ TÃ¡ch tá»« tiáº¿ng Viá»‡t
    â†“
[Embedding Layer] â†’ Chuyá»ƒn tá»« thÃ nh vector
    â†“
[12 Transformer Encoder Layers] â†’ Há»c context bidirectional
    â†“
[Classification Head] â†’ 3 neurons (Pos, Neg, Neu)
    â†“
[Softmax] â†’ XÃ¡c suáº¥t [0-1]
    â†“
Output: {POSITIVE: 0.7, NEGATIVE: 0.1, NEUTRAL: 0.2}
```

#### **B. ThÃ´ng sá»‘ ká»¹ thuáº­t**

| ThÃ´ng sá»‘ | GiÃ¡ trá»‹ | Ã nghÄ©a |
|----------|---------|---------|
| **Layers** | 12 | Sá»‘ lá»›p Transformer encoder |
| **Hidden size** | 768 | KÃ­ch thÆ°á»›c vector áº©n |
| **Attention heads** | 12 | Sá»‘ Ä‘áº§u attention trong má»—i lá»›p |
| **Parameters** | ~135M | Tá»•ng sá»‘ tham sá»‘ (triá»‡u) |
| **Vocabulary** | ~64K | Sá»‘ lÆ°á»£ng tokens trong tá»« Ä‘iá»ƒn |
| **Max length** | 256 | Äá»™ dÃ i tá»‘i Ä‘a (tokens) |

---

### ğŸ”¬ **3. QUY TRÃŒNH Xá»¬ LÃ CHI TIáº¾T**

#### **BÆ¯á»šC 1: TOKENIZATION (TÃ¡ch tá»«)**

```python
text = "Cá»• phiáº¿u FLC tÄƒng máº¡nh sau tin tÃ¡i cáº¥u trÃºc"

# PhoBERT sá»­ dá»¥ng BPE (Byte-Pair Encoding) tokenizer
tokens = tokenizer(text)
```

**Output:**
```python
{
    'input_ids': [0, 5432, 8901, 234, 7654, 3421, ...],  # ID cá»§a tá»«ng token
    'attention_mask': [1, 1, 1, 1, 1, 1, ...]          # Mask Ä‘á»ƒ phÃ¢n biá»‡t padding
}
```

**Äáº·c Ä‘iá»ƒm cá»§a Vietnamese Tokenizer:**
- Xá»­ lÃ½ **tiáº¿ng Viá»‡t cÃ³ dáº¥u** chÃ­nh xÃ¡c
- Hiá»ƒu **tá»« ghÃ©p** (cá»•_phiáº¿u, tÃ¡i_cáº¥u_trÃºc)
- Xá»­ lÃ½ **viáº¿t táº¯t** (VN, HOSE, HNX)
- Nháº­n diá»‡n **sá»‘** vÃ  **kÃ½ hiá»‡u Ä‘áº·c biá»‡t**

---

#### **BÆ¯á»šC 2: EMBEDDING (Chuyá»ƒn thÃ nh Vector)**

```python
# 3 loáº¡i embedding Ä‘Æ°á»£c cá»™ng láº¡i
Token Embedding:    [768-dim vector] # Ã nghÄ©a cá»§a tá»«
Position Embedding: [768-dim vector] # Vá»‹ trÃ­ trong cÃ¢u
Segment Embedding:  [768-dim vector] # Thuá»™c cÃ¢u nÃ o (náº¿u cÃ³ nhiá»u cÃ¢u)

Final Embedding = Token + Position + Segment
```

**VÃ­ dá»¥:**
```
"tÄƒng" â†’ [0.23, -0.45, 0.67, ..., 0.12]  (768 sá»‘)
"giáº£m" â†’ [-0.19, 0.52, -0.34, ..., -0.08] (768 sá»‘)
```

CÃ¡c tá»« cÃ³ nghÄ©a gáº§n nhau sáº½ cÃ³ vector gáº§n nhau trong khÃ´ng gian 768 chiá»u.

---

#### **BÆ¯á»šC 3: TRANSFORMER ENCODERS (12 táº§ng)**

Má»—i táº§ng Transformer thá»±c hiá»‡n:

##### **3a. Multi-Head Self-Attention**
```python
# Má»—i tá»« "nhÃ¬n" toÃ n bá»™ cÃ¢u Ä‘á»ƒ hiá»ƒu context

"FLC tÄƒng máº¡nh"
     â†“
Attention weights:
- "FLC" chÃº Ã½ nhiá»u Ä‘áº¿n "tÄƒng" (0.8)
- "tÄƒng" chÃº Ã½ nhiá»u Ä‘áº¿n "máº¡nh" (0.7)
- "máº¡nh" chÃº Ã½ ngÆ°á»£c láº¡i "tÄƒng" (0.6)
```

**12 attention heads** há»c cÃ¡c má»‘i quan há»‡ khÃ¡c nhau:
- Head 1: Quan há»‡ chá»§-vá»‹
- Head 2: Quan há»‡ tÃ­nh-danh tá»«
- Head 3: Ngá»¯ nghÄ©a tÃ­ch cá»±c/tiÃªu cá»±c
- ...

##### **3b. Feed-Forward Network**
```python
# Má»—i tá»« Ä‘Æ°á»£c xá»­ lÃ½ qua 2 lá»›p fully connected
FFN(x) = ReLU(xÂ·W1 + b1)Â·W2 + b2
```

##### **3c. Layer Normalization + Residual Connection**
```python
# á»”n Ä‘á»‹nh quÃ¡ trÃ¬nh training
output = LayerNorm(x + Attention(x))
output = LayerNorm(output + FFN(output))
```

**Sau 12 táº§ng**, má»—i tá»« cÃ³ vector **context-aware** (hiá»ƒu Ä‘Æ°á»£c nghÄ©a trong cÃ¢u).

---

#### **BÆ¯á»šC 4: CLASSIFICATION HEAD**

```python
# Láº¥y vector cá»§a token [CLS] (Ä‘áº¡i diá»‡n toÃ n bá»™ cÃ¢u)
cls_vector = last_hidden_state[0]  # Shape: (768,)

# ÄÆ°a qua fully connected layer
logits = Dense(3)(cls_vector)  # Shape: (3,) - 3 classes

# Output chÆ°a chuáº©n hÃ³a
logits = [2.3, -1.5, 0.8]  # Positive cao nháº¥t
```

---

#### **BÆ¯á»šC 5: SOFTMAX (Chuyá»ƒn thÃ nh xÃ¡c suáº¥t)**

```python
# CÃ´ng thá»©c Softmax
probs[i] = exp(logits[i]) / sum(exp(logits[j]) for j in range(3))

# VÃ­ dá»¥
logits = [2.3, -1.5, 0.8]
         â†“
probs = softmax(logits)
      = [0.71, 0.05, 0.24]  # Tá»•ng = 1.0
```

**Output cuá»‘i cÃ¹ng:**
```python
{
    'POSITIVE': 0.71,  # 71%
    'NEGATIVE': 0.05,  # 5%
    'NEUTRAL': 0.24    # 24%
}
```

---

### ğŸ’¡ **4. Äáº¶C ÄIá»‚M Ná»”I Báº¬T Cá»¦A PHOBERT**

#### **A. Bidirectional (Hai chiá»u)**

KhÃ¡c vá»›i mÃ´ hÃ¬nh truyá»n thá»‘ng (Ä‘á»c tá»« trÃ¡i â†’ pháº£i), BERT Ä‘á»c cáº£ hai chiá»u:

```
CÃ¢u: "Cá»• phiáº¿u FLC khÃ´ng tÄƒng mÃ  giáº£m máº¡nh"

Unidirectional (LSTM):
â†’ "khÃ´ng tÄƒng" â†’ Dá»± Ä‘oÃ¡n POSITIVE (SAI!)

Bidirectional (BERT):
â† "giáº£m máº¡nh" â† "khÃ´ng tÄƒng" â†’
â†’ Hiá»ƒu Ä‘Æ°á»£c "khÃ´ng tÄƒng" + "giáº£m" â†’ NEGATIVE (ÄÃšNG!)
```

#### **B. Pre-training + Fine-tuning**

```
Phase 1: PRE-TRAINING (VinAI Ä‘Ã£ lÃ m sáºµn)
- Corpus: 20GB text tiáº¿ng Viá»‡t (Wiki, bÃ¡o...)
- Task 1: Masked Language Modeling (MLM)
  "Cá»• phiáº¿u [MASK] tÄƒng máº¡nh" â†’ Dá»± Ä‘oÃ¡n "FLC"
- Task 2: Next Sentence Prediction (NSP)
  CÃ¢u A + CÃ¢u B cÃ³ liÃªn quan khÃ´ng?

Phase 2: FINE-TUNING (wonrax Ä‘Ã£ lÃ m)
- Dataset: Vietnamese sentiment annotated data
- Task: PhÃ¢n loáº¡i 3 nhÃ£n (Pos/Neg/Neu)
- Training: 10K+ tin tá»©c Ä‘Ã£ gÃ¡n nhÃ£n
```

#### **C. Transfer Learning**

PhoBERT Ä‘Ã£ há»c Ä‘Æ°á»£c:
- âœ… Ngá»¯ phÃ¡p tiáº¿ng Viá»‡t
- âœ… Quan há»‡ tá»« vá»±ng
- âœ… Ngá»¯ cáº£nh vÄƒn hÃ³a Viá»‡t Nam

â†’ Chá»‰ cáº§n **fine-tune** vá»›i Ã­t dá»¯ liá»‡u sentiment lÃ  Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao!

---

### ğŸ¯ **5. Táº I SAO PHOBERT Tá»T CHO TIN TÃ€I CHÃNH?**

#### **A. Hiá»ƒu tá»« vá»±ng chuyÃªn ngÃ nh**

```python
# PhoBERT há»c Ä‘Æ°á»£c:
"tÄƒng tráº§n" â†’ POSITIVE (technical term)
"vá» sÃ n" â†’ NEGATIVE (price floor)
"cá»• tá»©c" â†’ POSITIVE (dividend)
"thanh tra" â†’ NEGATIVE (investigation)
"tÃ¡i cáº¥u trÃºc" â†’ NEUTRAL/POSITIVE (context-dependent)
```

#### **B. Xá»­ lÃ½ phá»§ Ä‘á»‹nh phá»©c táº¡p**

```python
# MÃ´ hÃ¬nh cÅ© (rule-based):
"khÃ´ng tá»‘t" â†’ Count("khÃ´ng") â†’ NEGATIVE (ÄÃšNG)
"khÃ´ng giáº£m" â†’ Count("khÃ´ng") â†’ NEGATIVE (SAI!)

# PhoBERT:
"khÃ´ng giáº£m" â†’ Hiá»ƒu "khÃ´ng" + "giáº£m" â†’ POSITIVE âœ…
"khÃ´ng cÃ²n tÄƒng" â†’ Context-aware â†’ NEGATIVE âœ…
```

#### **C. Äá»™ chÃ­nh xÃ¡c cao**

Benchmark trÃªn Vietnamese sentiment:
```
Traditional ML (SVM):     ~75% accuracy
LSTM:                     ~82% accuracy
PhoBERT:                  ~92% accuracy âœ…
```

---

### âš™ï¸ **6. CODE WORKFLOW TRONG PROJECT**

```python
# 1. INITIALIZATION (Chá»‰ cháº¡y 1 láº§n)
tokenizer = AutoTokenizer.from_pretrained("wonrax/phobert-base-vietnamese-sentiment")
model = AutoModelForSequenceClassification.from_pretrained(...)
model.eval()  # Cháº¿ Ä‘á»™ inference (khÃ´ng training)

# 2. INFERENCE (Má»—i láº§n phÃ¢n tÃ­ch)
def analyze_sentiment(text):
    # a. Tokenization
    inputs = tokenizer(text, 
                      return_tensors="pt",    # PyTorch tensor
                      truncation=True,        # Cáº¯t náº¿u quÃ¡ dÃ i
                      padding=True)           # Padding Ä‘áº¿n max_length
    
    # b. Forward pass (khÃ´ng tÃ­nh gradient)
    with torch.no_grad():
        outputs = model(**inputs)
        # outputs.logits: Tensor([2.3, -1.5, 0.8])
    
    # c. Softmax
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    # probs: Tensor([0.71, 0.05, 0.24])
    
    # d. Map to labels
    label_map = model.config.id2label  # {0: 'NEG', 1: 'NEU', 2: 'POS'}
    result = {label_map[i]: probs[0][i].item() for i in range(3)}
    
    return result
```
