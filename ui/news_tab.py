# ======================================================
# üì∞ ui/news_tab.py ‚Äî Tab tin t·ª©c t·ª´ nhi·ªÅu ngu·ªìn
# ======================================================
"""
News Tab Module - Hi·ªÉn th·ªã tin t·ª©c ch·ª©ng kho√°n t·ª´ nhi·ªÅu ngu·ªìn

Features:
- RSS feed parsing (VnExpress, CafeF, VietStock)
- Web scraping (vnEconomy, Investing.com)
- AI sentiment analysis using PhoBERT
- Smart caching v√† retry logic
- Pagination support
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import feedparser
import json
import time
import math
import numbers
import re
import logging
from email.utils import parsedate_to_datetime
from typing import List, Dict, Optional, Tuple
from functools import lru_cache

# Import PhoBERT sentiment analysis
from models.sentiment_phobert import analyze_sentiment

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ======================================================
# üîß CONSTANTS & CONFIGURATION
# ======================================================

# Keywords ƒë·ªÉ l·ªçc tin t·ª©c v·ªÅ ch·ª©ng kho√°n Vi·ªát Nam
VN_STOCK_KEYWORDS = [
    "ch·ª©ng kho√°n", "th·ªã tr∆∞·ªùng vi·ªát nam", "th·ªã tr∆∞·ªùng ch·ª©ng kho√°n",
    "vn-index", "vnindex", "vn30", "vni", "hose", "hnx", "upcom",
    "vietstock", "doanh nghi·ªáp ni√™m y·∫øt", "c·ªï phi·∫øu",
    "ssi", "vcb", "vic", "vnm", "tcbs", "vcbs"
]

# Keywords ƒë·ªÉ lo·∫°i tr·ª´ tin t·ª©c kh√¥ng li√™n quan
EXCLUDED_TOPIC_KEYWORDS = [
    "crypto", "bitcoin", "ethereum", "blockchain", "forex", "fed",
    "nasdaq", "dow jones", "s&p", "us market", "wall street",
    "goldman sachs", "ch·ª©ng kho√°n m·ªπ", "tr√°i phi·∫øu m·ªπ",
    "ti·ªÅn ·∫£o", "ti·ªÅn ƒëi·ªán t·ª≠"
]

# RSS Feed URLs
RSS_FEEDS = {
    "vnexpress": ["https://vnexpress.net/rss/kinh-doanh.rss"],
    "cafef": ["https://cafef.vn/thi-truong-chung-khoan.rss"],
    "vietstock": [
        "https://vietstock.vn/830/chung-khoan/co-phieu.rss",
        "https://vietstock.vn/739/chung-khoan/giao-dich-noi-bo.rss",
        "https://vietstock.vn/741/chung-khoan/niem-yet.rss"
    ]
}

# Patterns
VNECONOMY_ARTICLE_SLUG = re.compile(r"^/[\w\-/]+-e\d+\.htm$")

# Request configuration
REQUEST_TIMEOUT = 15  # seconds
MAX_RETRIES = 3
RETRY_DELAY = 2  # seconds
CACHE_TTL = 300  # 5 minutes

# Headers cho web requests
DEFAULT_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache'
}

# ======================================================
# üîß HELPER FUNCTIONS - Date & Time
# ======================================================

def convert_relative_date(relative_date: str) -> datetime:
    """Chuy·ªÉn ƒë·ªïi th·ªùi gian t∆∞∆°ng ƒë·ªëi th√†nh th·ªùi gian th·ª±c"""
    try:
        if "minute" in relative_date:
            minutes = int(relative_date.split()[0])
            return datetime.now() - timedelta(minutes=minutes)
        elif "hour" in relative_date:
            hours = int(relative_date.split()[0])
            return datetime.now() - timedelta(hours=hours)
        elif "day" in relative_date:
            days = int(relative_date.split()[0])
            return datetime.now() - timedelta(days=days)
        else:
            return datetime.now()
    except Exception as e:
        st.warning(f"Error parsing date: {e}")
        return datetime.now()


def is_vietnam_stock_article(title: str, content: str) -> bool:
    """Ki·ªÉm tra b√†i vi·∫øt c√≥ li√™n quan ƒë·∫øn th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam."""
    combined_text = f"{title or ''} {content or ''}".lower()
    if any(excluded in combined_text for excluded in EXCLUDED_TOPIC_KEYWORDS):
        return False
    return any(keyword in combined_text for keyword in VN_STOCK_KEYWORDS)


def format_display_date(date_value):
    """ƒê·ªãnh d·∫°ng th·ªùi gian th√†nh chu·ªói th√¢n thi·ªán DD/MM/YYYY - HH:MM"""
    try:
        if isinstance(date_value, datetime):
            dt = date_value
        elif isinstance(date_value, numbers.Number):
            timestamp = float(date_value)
            if timestamp > 1e12:
                timestamp /= 1000  # vnstock tr·∫£ v·ªÅ millisecond
            dt = datetime.fromtimestamp(timestamp)
        elif isinstance(date_value, time.struct_time):
            dt = datetime.fromtimestamp(time.mktime(date_value))
        elif isinstance(date_value, str):
            stripped_value = date_value.strip()
            if stripped_value.isdigit():
                timestamp = float(stripped_value)
                if timestamp > 1e12:
                    timestamp /= 1000
                dt = datetime.fromtimestamp(timestamp)
            else:
                dt = parsedate_to_datetime(stripped_value)
        else:
            dt = datetime.now()

        if dt.tzinfo is not None:
            dt = dt.astimezone().replace(tzinfo=None)

        return dt.strftime("%d/%m/%Y - %H:%M")
    except Exception:
        if isinstance(date_value, str) and date_value:
            return date_value
        return datetime.now().strftime("%d/%m/%Y - %H:%M")


# ======================================================
# ü§ñ AI SENTIMENT ANALYSIS
# ======================================================

@st.cache_data(ttl=CACHE_TTL, show_spinner=False)
def get_ai_sentiment(text: str) -> Tuple[str, float]:
    """
    Ph√¢n t√≠ch sentiment s·ª≠ d·ª•ng PhoBERT model
    
    Args:
        text: VƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch
        
    Returns:
        Tuple[str, float]: (sentiment_label, confidence_score)
    """
    try:
        result = analyze_sentiment(text)
        
        if result and isinstance(result, dict):
            # PhoBERT tr·∫£ v·ªÅ dict v·ªõi keys nh∆∞ 'NEG', 'NEU', 'POS'
            # T√¨m label c√≥ score cao nh·∫•t
            label = max(result, key=result.get)
            score = result[label]
            
            # Map PhoBERT labels to our labels
            sentiment_map = {
                'POS': 'positive',
                'NEG': 'negative', 
                'NEU': 'neutral',
                'positive': 'positive',
                'negative': 'negative',
                'neutral': 'neutral'
            }
            
            return sentiment_map.get(label, 'neutral'), float(score)
        return 'neutral', 0.5
        
    except Exception as e:
        logger.warning(f"AI sentiment analysis failed: {e}. Falling back to keyword-based.")
        return get_keyword_based_sentiment(text)


def get_keyword_based_sentiment(text: str) -> Tuple[str, float]:
    """
    Ph√¢n t√≠ch sentiment d·ª±a tr√™n keywords (fallback method)
    
    Args:
        text: VƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch
        
    Returns:
        Tuple[str, float]: (sentiment_label, confidence_score)
    """
    positive_keywords = ["tƒÉng", "h·ªìi ph·ª•c", "l√£i", "tang", "hoi phuc", "lai", "t√≠ch c·ª±c", "kh·ªüi s·∫Øc"]
    negative_keywords = ["gi·∫£m", "b√°n th√°o", "l·ªó", "giam", "ban thao", "lo", "ti√™u c·ª±c", "s·ª•t gi·∫£m"]
    
    text_lower = text.lower()
    pos_count = sum(1 for kw in positive_keywords if kw in text_lower)
    neg_count = sum(1 for kw in negative_keywords if kw in text_lower)
    
    if pos_count > neg_count:
        return 'positive', min(0.6 + (pos_count * 0.1), 0.9)
    elif neg_count > pos_count:
        return 'negative', min(0.6 + (neg_count * 0.1), 0.9)
    else:
        return 'neutral', 0.5


def get_news_sentiment_styles(title: str, content: str, use_ai: bool = True) -> Dict[str, str]:
    """
    X√°c ƒë·ªãnh sentiment v√† style cho tin t·ª©c
    
    Args:
        title: Ti√™u ƒë·ªÅ tin t·ª©c
        content: N·ªôi dung tin t·ª©c
        use_ai: C√≥ s·ª≠ d·ª•ng AI sentiment analysis kh√¥ng
        
    Returns:
        Dict v·ªõi border, background, label, sentiment, confidence
    """
    # Ph√¢n t√≠ch sentiment d·ª±a tr√™n title
    text = title or content or ""
    
    # S·ª≠ d·ª•ng AI ho·∫∑c keyword-based sentiment
    if use_ai:
        sentiment, confidence = get_ai_sentiment(text)  # Ph√¢n t√≠ch to√†n b·ªô title
    else:
        sentiment, confidence = get_keyword_based_sentiment(text)
    
    styles = {
        "positive": {
            "border": "#22c55e",
            "background": "linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%)",
            "label": "Tin t√≠ch c·ª±c",
            "icon": "üìà"
        },
        "negative": {
            "border": "#ef4444",
            "background": "linear-gradient(135deg, #fee2e2 0%, #fecaca 100%)",
            "label": "Tin ti√™u c·ª±c",
            "icon": "üìâ"
        },
        "neutral": {
            "border": "#d97706",
            "background": "linear-gradient(135deg, #fef3c7 0%, #fde68a 100%)",
            "label": "Tin trung l·∫≠p",
            "icon": "üìä"
        }
    }
    
    result = styles[sentiment].copy()
    result['sentiment'] = sentiment
    result['confidence'] = confidence
    return result


# ======================================================
# üåê HTTP REQUEST UTILITIES
# ======================================================

def make_request_with_retry(url: str, headers: Dict = None, max_retries: int = MAX_RETRIES) -> requests.Response:
    """
    Th·ª±c hi·ªán HTTP request v·ªõi retry logic
    
    Args:
        url: URL c·∫ßn request
        headers: Custom headers
        max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
        
    Returns:
        Response object
        
    Raises:
        requests.RequestException: Khi t·∫•t c·∫£ retry ƒë·ªÅu th·∫•t b·∫°i
    """
    if headers is None:
        headers = DEFAULT_HEADERS.copy()
    
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            response = requests.get(
                url, 
                headers=headers, 
                timeout=REQUEST_TIMEOUT, 
                allow_redirects=True
            )
            response.raise_for_status()
            return response
            
        except requests.exceptions.Timeout as e:
            last_exception = e
            logger.warning(f"Timeout on attempt {attempt + 1}/{max_retries} for {url}")
            
        except requests.exceptions.HTTPError as e:
            # Don't retry on 4xx errors (client errors)
            if 400 <= e.response.status_code < 500:
                raise
            last_exception = e
            logger.warning(f"HTTP error {e.response.status_code} on attempt {attempt + 1}/{max_retries}")
            
        except requests.exceptions.ConnectionError as e:
            last_exception = e
            logger.warning(f"Connection error on attempt {attempt + 1}/{max_retries}")
        
        # Exponential backoff
        if attempt < max_retries - 1:
            sleep_time = RETRY_DELAY * (2 ** attempt)
            time.sleep(sleep_time)
    
    # All retries failed
    raise last_exception


# ======================================================
# üì° RSS FEED PARSER
# ======================================================

@st.cache_data(ttl=CACHE_TTL, show_spinner=False)
def fetch_rss_news(source: str = "vnexpress", max_articles: int = 5) -> List[Dict]:
    """
    L·∫•y tin t·ª©c t·ª´ RSS Feed v·ªõi error handling v√† retry logic
    
    Args:
        source: Ngu·ªìn tin (vnexpress, cafef, vietstock, vnEconomy)
        max_articles: S·ªë l∆∞·ª£ng b√†i vi·∫øt t·ªëi ƒëa
        
    Returns:
        List[Dict]: Danh s√°ch tin t·ª©c
    """
    # Special handling for vnEconomy - use web scraping instead
    if source == "vnEconomy":
        return scrape_vneconomy_news(max_articles)
    
    if source not in RSS_FEEDS:
        logger.error(f"Unknown news source: {source}")
        return []
    
    urls = RSS_FEEDS[source]
    aggregated_news = []
    errors = []

    # Try each URL and accumulate until we have enough articles
    for url_index, url in enumerate(urls):
        try:
            # Enhanced headers for RSS
            headers = DEFAULT_HEADERS.copy()
            headers['Accept'] = 'application/rss+xml, application/xml, text/xml, */*'
            
            # Fetch RSS with retry logic
            response = make_request_with_retry(url, headers=headers)
            
            # Parse RSS
            feed = feedparser.parse(response.content)
            
            # Check if feed has entries
            if not feed.entries:
                errors.append(f"Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt t·ª´ {url}")
                continue
            
            # Process each entry
            for entry in feed.entries:
                if len(aggregated_news) >= max_articles:
                    break
                    
                try:
                    # Extract title
                    title = entry.title if hasattr(entry, 'title') else "No Title"
                    link = entry.link if hasattr(entry, 'link') else ""
                    
                    # Parse date with multiple fallbacks
                    date = _extract_entry_date(entry)
                    
                    # Get content
                    content = _extract_entry_content(entry)
                    
                    # Normalize content length
                    normalized_content = content[:500] + "..." if len(content) > 500 else content
                    
                    # Filter Vietnam stock articles only
                    if not is_vietnam_stock_article(title, normalized_content):
                        continue

                    aggregated_news.append({
                        "title": title,
                        "date": date,
                        "content": normalized_content,
                        "link": link,
                        "source": source.upper()
                    })
                    
                except Exception as e:
                    logger.warning(f"Error processing entry: {e}")
                    continue
            
            if len(aggregated_news) >= max_articles:
                break
                
        except requests.exceptions.HTTPError as e:
            error_msg = f"HTTP {e.response.status_code}"
            errors.append(f"L·ªói HTTP t·ª´ {url}: {error_msg}")
            logger.error(f"HTTP error fetching {url}: {error_msg}")
            
        except requests.exceptions.Timeout:
            errors.append(f"Timeout khi t·∫£i {url}")
            logger.error(f"Timeout fetching {url}")
            
        except requests.exceptions.ConnectionError as e:
            errors.append(f"L·ªói k·∫øt n·ªëi ƒë·∫øn {url}")
            logger.error(f"Connection error fetching {url}: {e}")
            
        except Exception as e:
            errors.append(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)[:80]}")
            logger.error(f"Unexpected error fetching {url}: {e}")

    # Return results
    if aggregated_news:
        logger.info(f"Successfully fetched {len(aggregated_news)} articles from {source}")
        return aggregated_news[:max_articles]
    
    # Show errors if no articles found
    if errors:
        error_summary = f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i RSS t·ª´ {source}:\n" + "\n".join(f"‚Ä¢ {err}" for err in errors[:3])
        st.warning(error_summary)
    else:
        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i RSS t·ª´ {source}")
    
    return []


def _extract_entry_date(entry) -> str:
    """Extract and format date from RSS entry"""
    published_struct = getattr(entry, 'published_parsed', None)
    updated_struct = getattr(entry, 'updated_parsed', None)
    
    if published_struct:
        return format_display_date(published_struct)
    elif updated_struct:
        return format_display_date(updated_struct)
    elif hasattr(entry, 'published'):
        return format_display_date(entry.published)
    elif hasattr(entry, 'updated'):
        return format_display_date(entry.updated)
    else:
        return format_display_date(datetime.now())


def _extract_entry_content(entry) -> str:
    """Extract content from RSS entry"""
    if hasattr(entry, 'summary'):
        return BeautifulSoup(entry.summary, 'html.parser').get_text(strip=True)
    elif hasattr(entry, 'description'):
        return BeautifulSoup(entry.description, 'html.parser').get_text(strip=True)
    else:
        return "N·ªôi dung ƒëang ƒë∆∞·ª£c c·∫≠p nh·∫≠t..."


# ======================================================
# üï∑Ô∏è WEB SCRAPING
# ======================================================

@st.cache_data(ttl=CACHE_TTL, show_spinner=False)
def scrape_vneconomy_news(max_articles: int = 5) -> List[Dict]:
    """
    Web scraping cho vnEconomy v·ªõi improved error handling
    
    Args:
        max_articles: S·ªë l∆∞·ª£ng b√†i vi·∫øt t·ªëi ƒëa
        
    Returns:
        List[Dict]: Danh s√°ch tin t·ª©c
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive'
        }
        
        base_section = "https://vneconomy.vn/chung-khoan.htm"
        max_section_pages = 5  # crawl deeper pages to get ƒë·ªß b√†i li√™n quan ch·ª©ng kho√°n
        urls_to_try = []

        for page in range(1, max_section_pages + 1):
            if page == 1:
                urls_to_try.append(base_section)
            else:
                urls_to_try.append(f"{base_section}?p={page}")

        # Fallback pages b·ªï sung th√™m b·ªëi c·∫£nh kinh t·∫ø Vi·ªát Nam n·∫øu trang ch√≠nh thi·∫øu b√†i
        urls_to_try.extend([
            "https://vneconomy.vn/kinh-te.htm",
            "https://vneconomy.vn"
        ])
        
        collected_news = []
        seen_links = set()

        for base_url in urls_to_try:
            try:
                response = requests.get(base_url, headers=headers, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                page_news = []
                
                # Find article containers - vnEconomy uses different classes
                # Try multiple possible selectors
                article_selectors = [
                    'div.story',
                    'div.story-item',
                    'article.story',
                    'div.news-item',
                    'div.item-news'
                ]
                
                articles = []
                for selector in article_selectors:
                    articles = soup.select(selector)
                    if articles:
                        break
                
                if not articles:
                    # Fallback: find any links that look like articles
                    articles = soup.find_all('a', href=True)
                    articles = [a for a in articles if '/tin-tuc/' in a.get('href', '') or '/kinh-te/' in a.get('href', '')][:max_articles * 2]
                
                for article in articles[:max_articles * 3]:
                    if len(collected_news) >= max_articles:
                        break
                    
                    try:
                        # Extract title
                        title_elem = article.find('h3') or article.find('h2') or article.find('a')
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        if not title or len(title) < 10:
                            continue
                        
                        # Extract link
                        link_elem = article.find('a') if article.name != 'a' else article
                        link = link_elem.get('href', '') if link_elem else ''
                        if link and not link.startswith('http'):
                            link = f"https://vneconomy.vn{link}"
                        
                        # Extract date
                        time_elem = article.find('time') or article.find('span', class_=['time', 'date', 'published'])
                        raw_date = time_elem.get_text(strip=True) if time_elem else datetime.now()
                        date = format_display_date(raw_date) if raw_date else format_display_date(datetime.now())
                        
                        # Extract description
                        desc_elem = article.find('p') or article.find('div', class_=['description', 'desc', 'summary'])
                        content = desc_elem.get_text(strip=True) if desc_elem else "ƒê·ªçc th√™m t·∫°i vneconomy.vn"
                        
                        if len(content) < 20:
                            content = f"{title[:100]}... ƒê·ªçc th√™m t·∫°i vneconomy.vn"
                        
                        normalized_content = content[:500] + "..." if len(content) > 500 else content

                        passes_filter = is_vietnam_stock_article(title, normalized_content)
                        lower_text = f"{title} {normalized_content}".lower()
                        if not passes_filter:
                            if (link.startswith("https://vneconomy.vn/chung-khoan") or link.startswith("/chung-khoan") or "chung-khoan" in base_url.lower()) and not any(excluded in lower_text for excluded in EXCLUDED_TOPIC_KEYWORDS):
                                passes_filter = True
                        if not passes_filter:
                            continue

                        unique_key = link or title
                        if unique_key in seen_links:
                            continue
                        seen_links.add(unique_key)

                        page_news.append({
                            "title": title,
                            "date": date,
                            "content": normalized_content,
                            "link": link,
                            "source": "VNECONOMY "
                        })
                    except Exception:
                        continue
                
                if len(collected_news) + len(page_news) < max_articles:
                    for anchor in soup.find_all('a', href=True):
                        if len(collected_news) + len(page_news) >= max_articles:
                            break
                        raw_href = anchor.get('href', '')
                        if not raw_href or raw_href.startswith('javascript') or raw_href.startswith('#'):
                            continue
                        if not VNECONOMY_ARTICLE_SLUG.match(raw_href):
                            continue
                        anchor_title = anchor.get_text(strip=True)
                        if not anchor_title or len(anchor_title) < 10:
                            continue
                        link = raw_href if raw_href.startswith('http') else f"https://vneconomy.vn{raw_href}"
                        if link in seen_links:
                            continue

                        placeholder_content = f"Tin nhanh VnEconomy: {anchor_title}. ƒê·ªçc n·ªôi dung chi ti·∫øt tr√™n trang g·ªëc."
                        passes_filter = is_vietnam_stock_article(anchor_title, placeholder_content)
                        if not passes_filter:
                            lower_text = anchor_title.lower()
                            if (link.startswith("https://vneconomy.vn/chung-khoan") or raw_href.startswith("/chung-khoan") or "chung-khoan" in base_url.lower()) and not any(excluded in lower_text for excluded in EXCLUDED_TOPIC_KEYWORDS):
                                passes_filter = True
                        if not passes_filter:
                            continue

                        seen_links.add(link)
                        page_news.append({
                            "title": anchor_title,
                            "date": format_display_date(datetime.now()),
                            "content": placeholder_content,
                            "link": link,
                            "source": "VNECONOMY "
                        })

                if page_news:
                    collected_news.extend(page_news)
                    if len(collected_news) >= max_articles:
                        return collected_news[:max_articles]
                    
            except Exception:
                continue
        
        return collected_news
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ scrape vnEconomy: {str(e)[:80]}")
        return []


@st.cache_data(ttl=300, show_spinner=False)  # Cache 5 ph√∫t
def scrape_investing_news(page_num, max_articles=5):
    """
    Scrape tin t·ª©c t·ª´ Investing.com
    
    Args:
        page_num: S·ªë trang c·∫ßn crawl
        max_articles: S·ªë b√†i vi·∫øt t·ªëi ƒëa c·∫ßn l·∫•y
    
    Returns:
        List[dict]: Danh s√°ch tin t·ª©c
    """
    # URL ƒë√∫ng cho Investing.com stock market news
    if page_num == 1:
        url = "https://www.investing.com/news/stock-market-news"
    else:
        url = f"https://www.investing.com/news/stock-market-news/{page_num}"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15, verify=True)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        st.error(f"‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Investing.com: {str(e)[:100]}")
        st.info("üí° C√≥ th·ªÉ do: (1) M·∫°ng b·ªã ch·∫∑n, (2) Website ƒëang b·∫£o tr√¨, (3) C·∫ßn VPN")
        return []

    soup = BeautifulSoup(response.content, 'html.parser')
    articles = soup.find_all('div', class_='news-analysis-v2_content__z0iLP w-full text-xs sm:flex-1')

    news_data = []
    for article in articles:
        if len(news_data) >= max_articles:
            break
            
        try:
            # L·∫•y ti√™u ƒë·ªÅ
            title_elem = article.find(
                'a',
                class_='text-inv-blue-500 hover:text-inv-blue-500 hover:underline focus:text-inv-blue-500 focus:underline whitespace-normal text-sm font-bold leading-5 !text-[#181C21] sm:text-base sm:leading-6 lg:text-lg lg:leading-7'
            )
            if not title_elem:
                continue
            title = title_elem.get_text(strip=True)

            # L·∫•y th·ªùi gian
            time_elem = article.find('time')
            if time_elem:
                date_text = time_elem.get_text(strip=True)
                if "ago" in date_text:
                    date = format_display_date(convert_relative_date(date_text))
                else:
                    date = format_display_date(date_text)
            else:
                date = format_display_date(datetime.now())

            # L·∫•y li√™n k·∫øt b√†i vi·∫øt chi ti·∫øt
            link = title_elem.get('href', '')
            if link.startswith("http"):
                full_link = link
            else:
                full_link = f"https://www.investing.com{link}"

            # L·∫•y n·ªôi dung b√†i vi·∫øt chi ti·∫øt
            content = "Loading..."
            try:
                detail_response = requests.get(full_link, headers=headers, timeout=10)
                detail_response.raise_for_status()
                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                content_div = detail_soup.find('div', class_='article_WYSIWYG__O0uhw article_articlePage__UMz3q text-[18px] leading-8')
                content = content_div.get_text(strip=True) if content_div else "No Content Available"
            except requests.exceptions.RequestException as e:
                content = f"Error retrieving content: {e}"

            if not is_vietnam_stock_article(title, content):
                continue

            news_data.append({
                "title": title,
                "date": date,
                "content": content,
                "link": full_link
            })
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error processing article: {e}")
            continue

    return news_data


def render_pagination_controls(total_pages):
    """Hi·ªÉn th·ªã ƒëi·ªÅu h∆∞·ªõng trang ·ªü cu·ªëi tab"""
    st.divider()
    spacer_left, control_col, spacer_right = st.columns([1, 2, 1])

    with control_col:
        prev_col, info_col, next_col = st.columns([1, 1, 1], gap="small")

        prev_disabled = st.session_state.news_current_page <= 1
        next_disabled = st.session_state.news_current_page >= total_pages

        if prev_col.button("‚¨ÖÔ∏è", use_container_width=True, disabled=prev_disabled, key="news_prev_btn"):
            st.session_state.news_current_page -= 1
            st.rerun()

        info_col.markdown(
            f"<div style='text-align:center; font-size:16px; font-weight:600;'>Trang {st.session_state.news_current_page} / {total_pages}</div>",
            unsafe_allow_html=True
        )

        if next_col.button("‚û°Ô∏è", use_container_width=True, disabled=next_disabled, key="news_next_btn"):
            st.session_state.news_current_page += 1
            st.rerun()


# ======================================================
# üì∞ RENDER TAB NEWS
# ======================================================

def render(ticker: str = None):
    """
    Hi·ªÉn th·ªã tab tin t·ª©c t·ª´ nhi·ªÅu ngu·ªìn v·ªõi AI sentiment analysis
    
    Args:
        ticker: M√£ c·ªï phi·∫øu (optional, for future filtering)
    """
    st.header("üì∞ Tin t·ª©c Th·ªã tr∆∞·ªùng Ch·ª©ng kho√°n Vi·ªát Nam")
    
    # Ch·ªçn ngu·ªìn tin v√† settings
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        st.markdown("""
        <p style='color:#94a3b8'>
        Tin t·ª©c m·ªõi nh·∫•t v·ªÅ th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam v·ªõi ph√¢n t√≠ch sentiment AI.
        </p>
        """, unsafe_allow_html=True)
    
    with col2:
        news_source = st.selectbox(
            "üì° Ch·ªçn ngu·ªìn:",
            ["vnexpress", "cafef", "vietstock", "vnEconomy"],
            format_func=lambda x: {
                "vnexpress": "VnExpress",
                "cafef": "CafeF", 
                "vietstock": "VietStock",
                "vnEconomy": "VnEconomy"
            }.get(x, x),
            key="news_source_select"
        )
    
    with col3:
        use_ai_sentiment = st.checkbox(
            "ü§ñ AI Sentiment",
            value=True,
            help="S·ª≠ d·ª•ng PhoBERT ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c tin t·ª©c"
        )
    
    # Kh·ªüi t·∫°o session state
    if 'news_current_page' not in st.session_state:
        st.session_state.news_current_page = 1
    
    if 'last_news_source' not in st.session_state:
        st.session_state.last_news_source = news_source
    
    # Reset page khi ƒë·ªïi ngu·ªìn
    if st.session_state.last_news_source != news_source:
        st.session_state.news_current_page = 1
        st.session_state.last_news_source = news_source
    
    per_page = 5
    
    # Refresh button
    col_refresh, col_spacer = st.columns([1, 4])
    with col_refresh:
        if st.button("üîÑ L√†m m·ªõi", key="refresh_news", help="T·∫£i l·∫°i tin t·ª©c m·ªõi"):
            st.cache_data.clear()
            st.rerun()
    
    st.divider()
    
    # ======================================================
    # üìä L·∫§Y V√Ä HI·ªÇN TH·ªä TIN T·ª®C
    # ======================================================
    # Progress tracking
    progress_placeholder = st.empty()
    
    with progress_placeholder:
        with st.spinner(f"üîç ƒêang t·∫£i tin t·ª©c t·ª´ {news_source.upper()}..."):
            news = fetch_rss_news(news_source, max_articles=50)
    
    progress_placeholder.empty()
    
    if not news:
        st.error(f"‚ùå Kh√¥ng th·ªÉ t·∫£i tin t·ª©c t·ª´ ngu·ªìn {news_source.upper()}")
        
        # Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c
        with st.expander("üîß H∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c", expanded=True):
            st.markdown("""
            ### Nguy√™n nh√¢n c√≥ th·ªÉ:
            
            - üåê **K·∫øt n·ªëi m·∫°ng**: Ki·ªÉm tra internet c·ªßa b·∫°n
            - üö´ **Website ch·∫∑n**: Ngu·ªìn tin c√≥ th·ªÉ ch·∫∑n request t·ª± ƒë·ªông
            - üîí **Firewall/Antivirus**: C√≥ th·ªÉ ƒëang ch·∫∑n k·∫øt n·ªëi
            - ‚è±Ô∏è **Timeout**: Server ph·∫£n h·ªìi qu√° ch·∫≠m
            
            ### Gi·∫£i ph√°p:
            
            1. **Th·ª≠ ngu·ªìn kh√°c**: Ch·ªçn ngu·ªìn tin kh√°c trong dropdown ·ªü tr√™n
            2. **L√†m m·ªõi**: Click n√∫t "üîÑ L√†m m·ªõi" ·ªü tr√™n
            3. **Ki·ªÉm tra k·∫øt n·ªëi**: ƒê·∫£m b·∫£o internet ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng
            """)
        
        return  # D·ª´ng execution n·∫øu kh√¥ng c√≥ tin t·ª©c
    
    # Ph√¢n trang
    total_pages = max(1, math.ceil(len(news) / per_page))
    current_page = min(st.session_state.news_current_page, total_pages)
    
    if current_page != st.session_state.news_current_page:
        st.session_state.news_current_page = current_page
        st.rerun()
    
    start_idx = (current_page - 1) * per_page
    page_news = news[start_idx:start_idx + per_page]
    
    if not page_news and current_page > 1:
        st.session_state.news_current_page = 1
        st.rerun()
    
    # Hi·ªÉn th·ªã t·ª´ng b√†i vi·∫øt v·ªõi sentiment analysis
    for index, item in enumerate(page_news, start=start_idx + 1):
        sentiment_styles = get_news_sentiment_styles(
            item['title'], 
            item['content'],
            use_ai=use_ai_sentiment
        )
        
        border_color = sentiment_styles['border']
        background_style = sentiment_styles['background']
        sentiment_label = sentiment_styles['label']
        sentiment_icon = sentiment_styles.get('icon', 'üìä')
        confidence = sentiment_styles.get('confidence', 0.0)
        
        # T·∫°o title link
        title_link = f"<a href='{item['link']}' target='_blank' style='color:#0f172a; text-decoration:none; hover:text-decoration:underline;'>{item['title']}</a>"
        
        # Hi·ªÉn th·ªã sentiment badge v·ªõi confidence
        confidence_pct = int(confidence * 100)
        sentiment_badge = f"{sentiment_icon} {sentiment_label}"
        if use_ai_sentiment:
            sentiment_badge += f" ({confidence_pct}%)"

        with st.container():
            st.markdown(f"""
            <div style='
                background: {background_style};
                border-left: 5px solid {border_color};
                padding: 18px;
                border-radius: 10px;
                margin-bottom: 20px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                transition: transform 0.2s;
            '>
                <div style='display: flex; justify-content: space-between; align-items: flex-start; gap: 16px; margin-bottom: 12px;'>
                    <h4 style='color: #0f172a; margin: 0; flex: 1; line-height: 1.4;'>
                        {title_link}
                    </h4>
                    <span style='
                        font-size: 11px; 
                        font-weight: 600; 
                        color: {border_color}; 
                        padding: 6px 12px; 
                        border: 1.5px solid {border_color}; 
                        border-radius: 20px;
                        white-space: nowrap;
                        background: rgba(255,255,255,0.7);
                    '>
                        {sentiment_badge}
                    </span>
                </div>
                <p style='color: #6b7280; font-size: 13px; margin: 8px 0 0 0;'>
                    üìÖ <b>ƒêƒÉng l√∫c:</b> {item['date']} | üì∞ <b>Ngu·ªìn:</b> {item.get('source', news_source.upper())}
                </p>
            </div>
            """, unsafe_allow_html=True)

            # N·ªôi dung
            st.markdown(f"<p style='color:#ffffff; line-height:1.6;'>{item['content']}</p>", unsafe_allow_html=True)
            
            # Link ƒë·ªçc th√™m
            st.markdown("<br>", unsafe_allow_html=True)
    
    # Pagination controls
    render_pagination_controls(total_pages)